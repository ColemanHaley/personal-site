@article{schwartz-et-al-2020-polysynthetic,
    title = {Neural Polysynthetic Language Modeling},
    author = {Lane Schwartz and Francis Tyers and Lori Levin and Christo Kirov and Patrick Littell and Chi-kiu Lo and Emily Prud'hommeaux and Hyunji Hayley Park and Kenneth Steimel and Rebecca Knowles and Jeffrey Micher and Lonny Strunk and Han Liu and Coleman Haley and Katherine J. Zhang and Robbie Jimmerson and Vasilisa Andriyanets and Aldrian Obaja Muis and Naoki Otani and Jong Hyuk Park and Zhisong Zhang},
    month = may,
    year = {2020},
    venue = {Preprint},
    journal = {arXiv preprint arXiv:2005.05477},
    linkarXiv = {https://arxiv.org/abs/2005.05477},
    abstract = {Research in natural language processing commonly assumes that approaches that work well for English and and other widely-used languages are "language agnostic". In high-resource languages, especially those that are analytic, a common approach is to treat morphologically-distinct variants of a common root as completely independent word types. This assumes, that there are limited morphological inflections per root, and that the majority will appear in a large enough corpus, so that the model can adequately learn statistics about each form. Approaches like stemming, lemmatization, or subword segmentation are often used when either of those assumptions do not hold, particularly in the case of synthetic languages like Spanish or Russian that have more inflection than English.
In the literature, languages like Finnish or Turkish are held up as extreme examples of complexity that challenge common modelling assumptions. Yet, when considering all of the world's languages, Finnish and Turkish are closer to the average case. When we consider polysynthetic languages (those at the extreme of morphological complexity), approaches like stemming, lemmatization, or subword modelling may not suffice. These languages have very high numbers of hapax legomena, showing the need for appropriate morphological handling of words, without which it is not possible for a model to capture enough word statistics.
We examine the current state-of-the-art in language modelling, machine translation, and text prediction for four polysynthetic languages: Guaraní, St. Lawrence Island Yupik, Central Alaskan Yupik, and Inuktitut. We then propose a novel framework for language modelling that combines knowledge representations from finite-state morphological analyzers with Tensor Product Representations in order to enable neural language models capable of handling the full range of typologically variant languages.},
}

@inproceedings{haley-2020-bert,
    title = "This is a {BERT}. Now there are several of them. Can they generalize to novel words?",
    author = "Haley, Coleman",
	  venue = "BlackboxNLP 2020",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.blackboxnlp-1.31",
    doi = "10.18653/v1/2020.blackboxnlp-1.31",
    pages = "333--341",
    linkpaper = {https://www.aclweb.org/anthology/2020.blackboxnlp-1.31/},
    abstract = "Recently, large-scale pre-trained neural network models such as BERT have achieved many state-of-the-art results in natural language processing. Recent work has explored the linguistic capacities of thes models. However, no work has focused on the ability of these models to generalize these capacities to novel words. This type of generalization is exhibited by humans, and is intimately related to morphology{---}humans are in many cases able to identify inflections of novel words in the appropriate context. This type of morphological capacity has not been previously tested in BERT models, and is important for morphologically-rich languages, which are under-studied in the literature regarding BERT{'}s linguistic capacities. In this work, we investigate this by considering monolingual and multilingual BERT models{'} abilities to agree in number with novel plural words in English, French, German, Spanish, and Dutch. We find that many models are not able to reliably determine plurality of novel words, suggesting potential deficiencies in the morphological capacities of BERT models.",
}
@inproceedings{haley-smolensky-2020-invertible,
    title = "Invertible Tree Embeddings using a Cryptographic Role Embedding Scheme",
    author = "Haley, Coleman  and
      Smolensky, Paul",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
	venue = "COLING 2020",
    day = 8,
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.328",
    linkpaper = "https://www.aclweb.org/anthology/2020.blackboxnlp-1.31/",
    doi = "10.18653/v1/2020.coling-main.328",
    pages = "3671--3683",
    abstract = "We present a novel method for embedding trees in a vector space based on Tensor-Product Representations (TPRs) which allows for inversion: the retrieval of the original tree structure and nodes from the vectorial embedding. Unlike previous attempts, this does not come at the cost of intractable representation size; we utilize a method for non-exact inversion, showing that it works well when there is sufficient randomness in the representation scheme for simple data and providing an upper bound on its error. To handle the huge number of possible tree positions without memoizing position representation vectors, we present a method (Cryptographic Role Embedding) using cryptographic hashing algorithms that allows for the representation of unboundedly many positions. Through experiments on parse tree data, we show a 30,000-dimensional Cryptographic Role Embedding of trees can provide invertibility with error {\textless} 1{\%} that previous methods would require 8.6 {\mbox{$\times$}} 1057 dimensions to represent.",
}¨
@article{park-et-al-2020-matters,
      title = {Morphology Matters: {A} Multilingual Language Modeling Analysis}, 
      author = {Hyunji Hayley Park and Katherine J. Zhang and Coleman Haley and Kenneth Steimel and Han Liu and Lane Schwartz},
      prevenue = {To appear in},
      venue = {Transactions of the Association for Computational Linguistics},
      day = 12,
      month = dec,
      year = {2020},
      journal = {arXiv preprint arXiv:2012.06262},
      linkarXiv = {https://arxiv.org/abs/2012.06262},
      abstract = "Prior studies in multilingual language modeling (e.g., Cotterell et al., 2018; Mielke et al., 2019) disagree on whether or not inflectional morphology makes languages harder to model. We attempt to resolve the disagreement and extend those studies. We compile a larger corpus of 145 Bible translations in 92 languages and a larger number of typological features. We fill in missing typological data for several languages and consider corpus-based measures of morphological complexity in addition to expert-produced typological features. We find that several morphological measures are significantly associated with higher surprisal when LSTM models are trained with BPE-segmented data. We also investigate linguistically-motivated subword segmentation strategies like Morfessor and Finite-State Transducers (FSTs) and find that these segmentation strategies yield better performance and reduce the impact of a language's morphology on language modeling.",
}.
@inproceedings{haley-wilson-2021-unnatural,
    title = "Deep neural networks easily learn unnatural infixation and reduplication patterns",
    author = "Haley, Coleman  and
      Wilson, Colin",
    booktitle = "Proceedings of the Society for Computation in Linguistics",
  	venue = "SCIL 2021",
    month = feb,
    year = "2021",
    address = "Online",
    url = "https://scholarworks.umass.edu/scil/vol4/iss1/52/",
    eid = "52",
  	volume = "4",
    linkpaper = {https://scholarworks.umass.edu/scil/vol4/iss1/52/}, linkposter = {/documents/haley-wilson-2021-unnatural-poster.pdf},
    abstract = "Morphological patterns can involve simple concatenation of fixed strings (e.g., unkind, kindness) or ‘nonconcatenative’ processes such as infixation (e.g., Chamorro {\textit{l\_{um}iʔeʔ}} ‘saw (actor-focus)’, Topping, 1973) and reduplication (e.g., Amele {\textit{\_{ba}bagawen}} ‘as he came out’, Roberts, 1987), among many others (e.g., Anderson, 1992; Inkelas, 2014). Recent work has established that deep neural networks are capable of inducing both concatenative and nonconatenative patterns (e.g., Kann and Schütze, 2017; Nelson et al., 2020). In this paper, we verify that encoder-decoder networks can learn and generalize attested types of infixation and reduplication from modest training sets. We show further that the same networks readily learn many infixation and reduplication patterns that are unattested in natural languages, raising questions about their relationship to linguistic theory and viability as models of human learning.",
}
